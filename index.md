---
layout: default
title: "James Y. Huang"
---

My name is James Y. Huang. Iâ€™m a final-year Ph.D. student at University of Southern California (USC) advised by [Prof. Muhao Chen](https://muhaochen.github.io/) and [Prof. Fred Morstatter](https://fredzilla.github.io/). My research interest lies in natural language processing and machine learning, with a focus on robustness and controllability of LLMs and VLMs. Previously, I was a research intern at Microsoft Research, Adobe Research, Amazon AWS AI and Tencent AI Lab.

Before Ph.D., I received my master's degree from University of California, Los Angeles (UCLA) where I worked with [Prof. Kai-Wei Chang](http://web.cs.ucla.edu/~kwchang/). I got my bachelor's degree in [Engineering Science](https://engsci.utoronto.ca/program/what-is-engsci/) from University of Toronto.



## News

Our [paper](https://aclanthology.org/2023.emnlp-main.900.pdf) on learning compositional sentence representations won \\
:trophy:**<span style="color:orangered">Outstanding Paper Award</span>** at EMNLP 2023!      


## Selected Publications

[**Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration**](https://arxiv.org/pdf/2511.19417) \\
James Y. Huang, Sheng Zhang, Qianchu Liu, Guanghui Qin, Tinghui Zhu, Tristan Naumann, Muhao Chen, Hoifung Poon \\
<em>Preprint</em>


[**OmniStruct: Universal Text-to-Structure Generation across Diverse Schemas**](https://arxiv.org/pdf/2511.18335) \\
<u>James Y. Huang</u>, Wenxuan Zhou, Nan Xu, Fei Wang, Qin Liu, Sheng Zhang, Hoifung Poon, Muhao Chen \\
<em>Preprint</em>


[**MetaScale: Test-Time Scaling with Evolving Meta-Thoughts**](https://arxiv.org/pdf/2503.13447) \\
Qin Liu, Wenxuan Zhou, Nan Xu, <u>James Y. Huang</u>, Fei Wang, Sheng Zhang, Hoifung Poon, Muhao Chen \\
<em>Preprint</em>


[**DeAL: Decoding-time Alignment for Large Language Models**](https://arxiv.org/pdf/2402.06147) \\
<u>James Y. Huang</u>, Sailik Sengupta, Daniele Bonadiman, Yi-an Lai, Arshit Gupta, Nikolaos Pappas, Saab Mansour, Katrin Kirchoff, Dan Roth \\
<em>ACL 2025</em>


[**MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding**](https://arxiv.org/pdf/2406.09411) \\
Fei Wang, Xingyu Fu, <u>James Y. Huang</u>, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, Hsiang-Hui Liu, Pan Lu, Chunyuan Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng Zhang, Hoifung Poon, Muhao Chen \\
<em>ICLR 2025</em>


[**Offset Unlearning for Large Language Models**](https://arxiv.org/pdf/2404.11045) \\
<u>James Y. Huang</u>, Wenxuan Zhou, Fei Wang, Fred Morstatter, Sheng Zhang, Hoifung Poon, Muhao Chen \\
<em>TMLR 2025</em>


[**mDPO: Conditional Preference Optimization for Multimodal Large Language Models**](https://arxiv.org/pdf/2406.11839) \\
Fei Wang, Wenxuan Zhou, <u>James Y. Huang</u>, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen \\
<em>EMNLP 2024</em>


[**Contrastive Instruction Tuning**](https://arxiv.org/pdf/2402.11138) \\
Tianyi Yan, Fei Wang, <u>James Y. Huang</u>, Wenxuan Zhou, Fan Yin, Aram Galstyan, Wenpeng Yin, Muhao Chen \\
<em>ACL 2024 (Findings)</em>


[**Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations**](https://arxiv.org/pdf/2305.14599) \\
<u>James Y. Huang</u>, Wenlin Yao, Kaiqiang Song, Hongming Zhang, Muhao Chen, Dong Yu \\
<em>EMNLP 2023</em> :trophy:**<span style="color:orangered">Outstanding Paper Award</span>**


[**Robust Natural Language Understanding with Residual Attention Debiasing**](https://arxiv.org/pdf/2305.17627) \\
Fei Wang, <u>James Y. Huang</u>, Tianyi Yan, Wenxuan Zhou, Muhao Chen \\
<em>ACL 2023 (Findings)</em>


[**Parameter-Efficient Tuning with Special Token Adaptation**](https://arxiv.org/pdf/2210.04382) \\
Xiaocong Yang, <u>James Y. Huang</u>, Wenxuan Zhou, Muhao Chen \\
<em>EACL 2023</em>


[**Unified Semantic Typing with Meaningful Label Inference**](https://arxiv.org/pdf/2205.01826) \\
<u>James Y. Huang</u>, Bangzheng Li, Jiashu Xu, Muhao Chen \\
<em>NAACL 2022</em>


[**Disentangling Semantics and Syntax in Sentence Embeddings with Pre-trained Language Models**](https://arxiv.org/pdf/2104.05115) \\
<u>James Y. Huang</u>, Kuan-Hao Huang, Kai-Wei Chang \\
<em>NAACL 2021</em>





